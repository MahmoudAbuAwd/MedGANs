{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, ReLU, LeakyReLU, BatchNormalization, Input, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "x0gFNKvEPtO8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = keras.Sequential()\n",
        "    result.add(Conv2D(filters, size, strides=2, padding='same',\n",
        "                      kernel_initializer=initializer, use_bias=False))\n",
        "    if apply_batchnorm:\n",
        "        result.add(BatchNormalization())\n",
        "    result.add(LeakyReLU())\n",
        "    return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = keras.Sequential()\n",
        "    result.add(Conv2DTranspose(filters, size, strides=2, padding='same',\n",
        "                               kernel_initializer=initializer, use_bias=False))\n",
        "    result.add(BatchNormalization())\n",
        "    if apply_dropout:\n",
        "        result.add(keras.layers.Dropout(0.5))\n",
        "    result.add(ReLU())\n",
        "    return result\n",
        "\n",
        "def Generator():\n",
        "    inputs = Input(shape=[256, 256, 3])\n",
        "\n",
        "    down_stack = [\n",
        "        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
        "        downsample(128, 4), # (bs, 64, 64, 128)\n",
        "        downsample(256, 4), # (bs, 32, 32, 256)\n",
        "        downsample(512, 4), # (bs, 16, 16, 512)\n",
        "        downsample(512, 4), # (bs, 8, 8, 512)\n",
        "        downsample(512, 4), # (bs, 4, 4, 512)\n",
        "        downsample(512, 4), # (bs, 2, 2, 512)\n",
        "        downsample(512, 4), # (bs, 1, 1, 512)\n",
        "    ]\n",
        "\n",
        "    up_stack = [\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 512)\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 512)\n",
        "        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 512)\n",
        "        upsample(512, 4), # (bs, 16, 16, 512)\n",
        "        upsample(256, 4), # (bs, 32, 32, 256)\n",
        "        upsample(128, 4), # (bs, 64, 64, 128)\n",
        "        upsample(64, 4),  # (bs, 128, 128, 64)\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    last = Conv2DTranspose(3, 4, strides=2, padding='same', kernel_initializer=initializer, activation='tanh') # (bs, 256, 256, 3)\n",
        "\n",
        "    x = inputs\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = Concatenate()([x, skip])\n",
        "\n",
        "    x = last(x)\n",
        "    return Model(inputs=inputs, outputs=x)\n",
        "\n",
        "# Instantiate two generators\n",
        "G_A2B = Generator()  # Generator for A (real) to B (synthetic)\n",
        "G_B2A = Generator()  # Generator for B (synthetic) to A (real)\n"
      ],
      "metadata": {
        "id": "iu4HhZR3Pu6X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Discriminator():\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    inp = Input(shape=[256, 256, 3], name='input_image')\n",
        "\n",
        "    down1 = downsample(64, 4, False)(inp)  # (bs, 128, 128, 64)\n",
        "    down2 = downsample(128, 4)(down1)      # (bs, 64, 64, 128)\n",
        "    down3 = downsample(256, 4)(down2)      # (bs, 32, 32, 256)\n",
        "\n",
        "    zero_pad1 = keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n",
        "    conv = Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n",
        "    batchnorm1 = BatchNormalization()(conv)\n",
        "    leaky_relu = LeakyReLU()(batchnorm1)\n",
        "\n",
        "    zero_pad2 = keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n",
        "    last = Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n",
        "\n",
        "    return Model(inputs=inp, outputs=last)\n",
        "\n",
        "# Instantiate two discriminators\n",
        "D_A = Discriminator()  # Discriminator for real domain A\n",
        "D_B = Discriminator()  # Discriminator for synthetic domain B\n"
      ],
      "metadata": {
        "id": "XDmte3LhP0tV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_obj = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real, generated):\n",
        "    real_loss = loss_obj(tf.ones_like(real), real)\n",
        "    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
        "    total_loss = real_loss + generated_loss\n",
        "    return total_loss * 0.5\n",
        "\n",
        "def generator_loss(generated):\n",
        "    return loss_obj(tf.ones_like(generated), generated)\n",
        "\n",
        "def cycle_loss(real_image, cycled_image, lambda_cycle=10):\n",
        "    loss = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "    return lambda_cycle * loss\n",
        "\n",
        "def identity_loss(real_image, same_image, lambda_identity=5):\n",
        "    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
        "    return lambda_identity * loss\n"
      ],
      "metadata": {
        "id": "zeH-3QwzP6BG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizers\n",
        "generator_optimizer = Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "# Training Step\n",
        "@tf.function\n",
        "def train_step(real_A, real_B):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # Generate fake images\n",
        "        fake_B = G_A2B(real_A, training=True)\n",
        "        cycled_A = G_B2A(fake_B, training=True)\n",
        "\n",
        "        fake_A = G_B2A(real_B, training=True)\n",
        "        cycled_B = G_A2B(fake_A, training=True)\n",
        "\n",
        "        # Identity mapping\n",
        "        same_A = G_B2A(real_A, training=True)\n",
        "        same_B = G_A2B(real_B, training=True)\n",
        "\n",
        "        # Discriminator predictions\n",
        "        disc_real_A = D_A(real_A, training=True)\n",
        "        disc_fake_A = D_A(fake_A, training=True)\n",
        "\n",
        "        disc_real_B = D_B(real_B, training=True)\n",
        "        disc_fake_B = D_B(fake_B, training=True)\n",
        "\n",
        "        # Generator losses\n",
        "        G_A2B_loss = generator_loss(disc_fake_B)\n",
        "        G_B2A_loss = generator_loss(disc_fake_A)\n",
        "\n",
        "        # Cycle-consistency losses\n",
        "        total_cycle_loss = cycle_loss(real_A, cycled_A) + cycle_loss(real_B, cycled_B)\n",
        "\n",
        "        # Identity losses\n",
        "        total_identity_loss = identity_loss(real_A, same_A) + identity_loss(real_B, same_B)\n",
        "\n",
        "        # Total generator loss\n",
        "        total_G_A2B_loss = G_A2B_loss + total_cycle_loss + total_identity_loss\n",
        "        total_G_B2A_loss = G_B2A_loss + total_cycle_loss + total_identity_loss\n",
        "\n",
        "        # Discriminator losses\n",
        "        D_A_loss = discriminator_loss(disc_real_A, disc_fake_A)\n",
        "        D_B_loss = discriminator_loss(disc_real_B, disc_fake_B)\n",
        "\n",
        "    # Calculate gradients\n",
        "    G_A2B_gradients = tape.gradient(total_G_A2B_loss, G_A2B.trainable_variables)\n",
        "    G_B2A_gradients = tape.gradient(total_G_B2A_loss, G_B2A.trainable_variables)\n",
        "\n",
        "    D_A_gradients = tape.gradient(D_A_loss, D_A.trainable_variables)\n",
        "    D_B_gradients = tape.gradient(D_B_loss, D_B.trainable_variables)\n",
        "\n",
        "    # Apply gradients\n",
        "    generator_optimizer.apply_gradients(zip(G_A2B_gradients, G_A2B.trainable_variables))\n",
        "    generator_optimizer.apply_gradients(zip(G_B2A_gradients, G_B2A.trainable_variables))\n",
        "\n",
        "    discriminator_optimizer.apply_gradients(zip(D_A_gradients, D_A.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(D_B_gradients, D_B.trainable_variables))\n",
        "\n",
        "    return total_G_A2B_loss, total_G_B2A_loss, D_A_loss, D_B_loss\n"
      ],
      "metadata": {
        "id": "UmCutsgUP6yJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of loading data (customize for your dataset)\n",
        "def load_image(file_path):\n",
        "    image = tf.io.read_file(file_path)\n",
        "    image = tf.image.decode_jpeg(image)\n",
        "    image = tf.image.resize(image, [256, 256])\n",
        "    image = (image / 127.5) - 1  # Normalize to [-1, 1]\n",
        "    return image\n",
        "\n",
        "# Use the tf.data API to create datasets for real_A and real_B\n",
        "# Replace with actual file paths for brain tumor images\n",
        "real_A_paths = [\"/content/image_dataset/1\"]\n",
        "real_B_paths = [\"/content/image_dataset/3\"]\n",
        "\n",
        "dataset_A = tf.data.Dataset.from_tensor_slices(real_A_paths).map(load_image).batch(1)\n",
        "dataset_B = tf.data.Dataset.from_tensor_slices(real_B_paths).map(load_image).batch(1)\n",
        "\n",
        "# Train for a number of epochs\n",
        "EPOCHS = 100\n",
        "for epoch in range(EPOCHS):\n",
        "    for real_A, real_B in zip(dataset_A, dataset_B):\n",
        "        G_A2B_loss, G_B2A_loss, D_A_loss, D_B_loss = train_step(real_A, real_B)\n",
        "        print(f'Epoch {epoch} - Generator Loss: {G_A2B_loss.numpy()}, {G_B2A_loss.numpy()}')\n"
      ],
      "metadata": {
        "id": "xZaz8iPrQHYn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}