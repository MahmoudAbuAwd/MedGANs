{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8HmkDr95xgo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from math import log2, sqrt\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"rm1000/brain-tumor-mri-scans\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfvLnp6X7Zlr",
        "outputId": "e5a6e16c-21d4-4eaf-c772-258278708aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/rm1000/brain-tumor-mri-scans?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 236M/236M [00:01<00:00, 191MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/rm1000/brain-tumor-mri-scans/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XHaQ1gzv7fDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip \"/content/drive/MyDrive/MEDGAN/archive (16).zip\" -d /content/extracted_files"
      ],
      "metadata": {
        "id": "tf0ZD-648WE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Hyperparameters\n",
        "\n",
        "DATASET = \"/root/.cache/kagglehub/datasets/rm1000/brain-tumor-mri-scans/versions/1\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "EPOCHS = 300\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 32\n",
        "LOG_RESOLUTION = 7 #for 128*128\n",
        "Z_DIM = 256\n",
        "W_DIM = 256\n",
        "LAMBDA_GP = 10"
      ],
      "metadata": {
        "id": "sPTHi2yz53sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader():\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((2 ** LOG_RESOLUTION, 2 ** LOG_RESOLUTION)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.Normalize(\n",
        "                [0.5, 0.5, 0.5],\n",
        "                [0.5, 0.5, 0.5],\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "    )\n",
        "    return loader"
      ],
      "metadata": {
        "id": "uzJqZftL55uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, z_dim, w_dim):\n",
        "        super().__init__()\n",
        "        self.mapping = nn.Sequential(\n",
        "            EqualizedLinear(z_dim, w_dim),\n",
        "            nn.ReLU(),\n",
        "            EqualizedLinear(z_dim, w_dim),\n",
        "            nn.ReLU(),\n",
        "            EqualizedLinear(z_dim, w_dim),\n",
        "            nn.ReLU(),\n",
        "            EqualizedLinear(z_dim, w_dim),\n",
        "            nn.ReLU(),\n",
        "            EqualizedLinear(z_dim, w_dim),\n",
        "            nn.ReLU(),\n",
        "            EqualizedLinear(z_dim, w_dim),\n",
        "            nn.ReLU(),\n",
        "            EqualizedLinear(z_dim, w_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + 1e-8)  # for PixelNorm\n",
        "        return self.mapping(x)\n"
      ],
      "metadata": {
        "id": "ogvWwBNh58P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, log_resolution, W_DIM, n_features = 32, max_features = 256):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 2, -1, -1)]\n",
        "        self.n_blocks = len(features)\n",
        "\n",
        "        self.initial_constant = nn.Parameter(torch.randn((1, features[0], 4, 4)))\n",
        "\n",
        "        self.style_block = StyleBlock(W_DIM, features[0], features[0])\n",
        "        self.to_rgb = ToRGB(W_DIM, features[0])\n",
        "\n",
        "        blocks = [GeneratorBlock(W_DIM, features[i - 1], features[i]) for i in range(1, self.n_blocks)]\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "\n",
        "    def forward(self, w, input_noise):\n",
        "\n",
        "        batch_size = w.shape[1]\n",
        "\n",
        "        x = self.initial_constant.expand(batch_size, -1, -1, -1)\n",
        "        x = self.style_block(x, w[0], input_noise[0][1])\n",
        "        rgb = self.to_rgb(x, w[0])\n",
        "\n",
        "        for i in range(1, self.n_blocks):\n",
        "            x = F.interpolate(x, scale_factor=2, mode=\"bilinear\")\n",
        "            x, rgb_new = self.blocks[i - 1](x, w[i], input_noise[i])\n",
        "            rgb = F.interpolate(rgb, scale_factor=2, mode=\"bilinear\") + rgb_new\n",
        "\n",
        "        return torch.tanh(rgb)"
      ],
      "metadata": {
        "id": "8CFR7ZA16G-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeneratorBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, W_DIM, in_features, out_features):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.style_block1 = StyleBlock(W_DIM, in_features, out_features)\n",
        "        self.style_block2 = StyleBlock(W_DIM, out_features, out_features)\n",
        "\n",
        "        self.to_rgb = ToRGB(W_DIM, out_features)\n",
        "\n",
        "    def forward(self, x, w, noise):\n",
        "\n",
        "        x = self.style_block1(x, w, noise[0])\n",
        "        x = self.style_block2(x, w, noise[1])\n",
        "\n",
        "        rgb = self.to_rgb(x, w)\n",
        "\n",
        "        return x, rgb\n"
      ],
      "metadata": {
        "id": "61zO7_Fz6Kh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, W_DIM, in_features, out_features):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.to_style = EqualizedLinear(W_DIM, in_features, bias=1.0)\n",
        "        self.conv = Conv2dWeightModulate(in_features, out_features, kernel_size=3)\n",
        "        self.scale_noise = nn.Parameter(torch.zeros(1))\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "\n",
        "        self.activation = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "    def forward(self, x, w, noise):\n",
        "\n",
        "        s = self.to_style(w)\n",
        "        x = self.conv(x, s)\n",
        "        if noise is not None:\n",
        "            x = x + self.scale_noise[None, :, None, None] * noise\n",
        "        return self.activation(x + self.bias[None, :, None, None])"
      ],
      "metadata": {
        "id": "g7zWB1Rk6NYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToRGB(nn.Module):\n",
        "\n",
        "    def __init__(self, W_DIM, features):\n",
        "\n",
        "        super().__init__()\n",
        "        self.to_style = EqualizedLinear(W_DIM, features, bias=1.0)\n",
        "\n",
        "        self.conv = Conv2dWeightModulate(features, 3, kernel_size=1, demodulate=False)\n",
        "        self.bias = nn.Parameter(torch.zeros(3))\n",
        "        self.activation = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "    def forward(self, x, w):\n",
        "\n",
        "        style = self.to_style(w)\n",
        "        x = self.conv(x, style)\n",
        "        return self.activation(x + self.bias[None, :, None, None])"
      ],
      "metadata": {
        "id": "gv0nnI036U8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2dWeightModulate(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features, kernel_size,\n",
        "                 demodulate = True, eps = 1e-8):\n",
        "\n",
        "        super().__init__()\n",
        "        self.out_features = out_features\n",
        "        self.demodulate = demodulate\n",
        "        self.padding = (kernel_size - 1) // 2\n",
        "\n",
        "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, s):\n",
        "\n",
        "        b, _, h, w = x.shape\n",
        "\n",
        "        s = s[:, None, :, None, None]\n",
        "        weights = self.weight()[None, :, :, :, :]\n",
        "        weights = weights * s\n",
        "\n",
        "        if self.demodulate:\n",
        "            sigma_inv = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + self.eps)\n",
        "            weights = weights * sigma_inv\n",
        "\n",
        "        x = x.reshape(1, -1, h, w)\n",
        "\n",
        "        _, _, *ws = weights.shape\n",
        "        weights = weights.reshape(b * self.out_features, *ws)\n",
        "\n",
        "        x = F.conv2d(x, weights, padding=self.padding, groups=b)\n",
        "\n",
        "        return x.reshape(-1, self.out_features, h, w)\n"
      ],
      "metadata": {
        "id": "fQEKg7OL6X9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, log_resolution, n_features = 64, max_features = 256):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 1)]\n",
        "\n",
        "        self.from_rgb = nn.Sequential(\n",
        "            EqualizedConv2d(3, n_features, 1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "        n_blocks = len(features) - 1\n",
        "        blocks = [DiscriminatorBlock(features[i], features[i + 1]) for i in range(n_blocks)]\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "        final_features = features[-1] + 1\n",
        "        self.conv = EqualizedConv2d(final_features, final_features, 3)\n",
        "        self.final = EqualizedLinear(2 * 2 * final_features, 1)\n",
        "\n",
        "    def minibatch_std(self, x):\n",
        "        batch_statistics = (\n",
        "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
        "        )\n",
        "        return torch.cat([x, batch_statistics], dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.from_rgb(x)\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        x = self.minibatch_std(x)\n",
        "        x = self.conv(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.final(x)"
      ],
      "metadata": {
        "id": "C-ve-bxN6Yqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscriminatorBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.residual = nn.Sequential(nn.AvgPool2d(kernel_size=2, stride=2), # down sampling using avg pool\n",
        "                                      EqualizedConv2d(in_features, out_features, kernel_size=1))\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            EqualizedConv2d(in_features, in_features, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            EqualizedConv2d(in_features, out_features, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "\n",
        "        self.down_sample = nn.AvgPool2d(\n",
        "            kernel_size=2, stride=2\n",
        "        )  # down sampling using avg pool\n",
        "\n",
        "        self.scale = 1 / sqrt(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.residual(x)\n",
        "\n",
        "        x = self.block(x)\n",
        "        x = self.down_sample(x)\n",
        "\n",
        "        return (x + residual) * self.scale"
      ],
      "metadata": {
        "id": "cUI0isjH6ekJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EqualizedLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias = 0.):\n",
        "\n",
        "        super().__init__()\n",
        "        self.weight = EqualizedWeight([out_features, in_features])\n",
        "        self.bias = nn.Parameter(torch.ones(out_features) * bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return F.linear(x, self.weight(), bias=self.bias)"
      ],
      "metadata": {
        "id": "11BqCyh66hfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EqualizedConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features,\n",
        "                 kernel_size, padding = 0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.padding = padding\n",
        "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n",
        "        self.bias = nn.Parameter(torch.ones(out_features))\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return F.conv2d(x, self.weight(), bias=self.bias, padding=self.padding)"
      ],
      "metadata": {
        "id": "KAVu7SXf6lTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EqualizedWeight(nn.Module):\n",
        "\n",
        "    def __init__(self, shape):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.c = 1 / sqrt(np.prod(shape[1:]))\n",
        "        self.weight = nn.Parameter(torch.randn(shape))\n",
        "\n",
        "    def forward(self):\n",
        "        return self.weight * self.c"
      ],
      "metadata": {
        "id": "jwfD_VCm6rcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PathLengthPenalty(nn.Module):\n",
        "\n",
        "    def __init__(self, beta):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.beta = beta\n",
        "        self.steps = nn.Parameter(torch.tensor(0.), requires_grad=False)\n",
        "\n",
        "        self.exp_sum_a = nn.Parameter(torch.tensor(0.), requires_grad=False)\n",
        "\n",
        "    def forward(self, w, x):\n",
        "\n",
        "        device = x.device\n",
        "        image_size = x.shape[2] * x.shape[3]\n",
        "        y = torch.randn(x.shape, device=device)\n",
        "\n",
        "        output = (x * y).sum() / sqrt(image_size)\n",
        "        sqrt(image_size)\n",
        "\n",
        "        gradients, *_ = torch.autograd.grad(outputs=output,\n",
        "                                            inputs=w,\n",
        "                                            grad_outputs=torch.ones(output.shape, device=device),\n",
        "                                            create_graph=True)\n",
        "\n",
        "        norm = (gradients ** 2).sum(dim=2).mean(dim=1).sqrt()\n",
        "\n",
        "        if self.steps > 0:\n",
        "\n",
        "            a = self.exp_sum_a / (1 - self.beta ** self.steps)\n",
        "\n",
        "            loss = torch.mean((norm - a) ** 2)\n",
        "        else:\n",
        "            loss = norm.new_tensor(0)\n",
        "\n",
        "        mean = norm.mean().detach()\n",
        "        self.exp_sum_a.mul_(self.beta).add_(mean, alpha=1 - self.beta)\n",
        "        self.steps.add_(1.)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "aSRYDRq56uBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(critic, real, fake,device=\"cpu\"):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
        "    interpolated_images.requires_grad_(True)\n",
        "\n",
        "    # Calculate critic scores\n",
        "    mixed_scores = critic(interpolated_images)\n",
        "\n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty\n"
      ],
      "metadata": {
        "id": "cpYNnzDL6upr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_w(batch_size):\n",
        "\n",
        "    z = torch.randn(batch_size, W_DIM).to(DEVICE)\n",
        "    w = mapping_network(z)\n",
        "    return w[None, :, :].expand(LOG_RESOLUTION, -1, -1)\n"
      ],
      "metadata": {
        "id": "kVF0NQWF6x_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_noise(batch_size):\n",
        "\n",
        "        noise = []\n",
        "        resolution = 4\n",
        "\n",
        "        for i in range(LOG_RESOLUTION):\n",
        "            if i == 0:\n",
        "                n1 = None\n",
        "            else:\n",
        "                n1 = torch.randn(batch_size, 1, resolution, resolution, device=DEVICE)\n",
        "            n2 = torch.randn(batch_size, 1, resolution, resolution, device=DEVICE)\n",
        "\n",
        "            noise.append((n1, n2))\n",
        "\n",
        "            resolution *= 2\n",
        "\n",
        "        return noise"
      ],
      "metadata": {
        "id": "NgBHCLe760i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_examples(gen, epoch, n=100):\n",
        "\n",
        "    gen.eval()\n",
        "    alpha = 1.0\n",
        "    for i in range(n):\n",
        "        with torch.no_grad():\n",
        "            w     = get_w(1)\n",
        "            noise = get_noise(1)\n",
        "            img = gen(w, noise)\n",
        "            if not os.path.exists(f'saved_examples/epoch{epoch}'):\n",
        "                os.makedirs(f'saved_examples/epoch{epoch}')\n",
        "            save_image(img*0.5+0.5, f\"saved_examples/epoch{epoch}/img_{i}.png\")\n",
        "\n",
        "    gen.train()"
      ],
      "metadata": {
        "id": "6nD0c02K63dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(\n",
        "    critic,\n",
        "    gen,\n",
        "    path_length_penalty,\n",
        "    loader,\n",
        "    opt_critic,\n",
        "    opt_gen,\n",
        "    opt_mapping_network,\n",
        "):\n",
        "    loop = tqdm(loader, leave=True)\n",
        "\n",
        "    for batch_idx, (real, _) in enumerate(loop):\n",
        "        real = real.to(DEVICE)\n",
        "        cur_batch_size = real.shape[0]\n",
        "\n",
        "        w     = get_w(cur_batch_size)\n",
        "        noise = get_noise(cur_batch_size)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            fake = gen(w, noise)\n",
        "            critic_fake = critic(fake.detach())\n",
        "\n",
        "            critic_real = critic(real)\n",
        "            gp = gradient_penalty(critic, real, fake, device=DEVICE)\n",
        "            loss_critic = (\n",
        "                -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
        "                + LAMBDA_GP * gp\n",
        "                + (0.001 * torch.mean(critic_real ** 2))\n",
        "            )\n",
        "\n",
        "        critic.zero_grad()\n",
        "        loss_critic.backward()\n",
        "        opt_critic.step()\n",
        "\n",
        "        gen_fake = critic(fake)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "\n",
        "        if batch_idx % 16 == 0:\n",
        "            plp = path_length_penalty(w, fake)\n",
        "            if not torch.isnan(plp):\n",
        "                loss_gen = loss_gen + plp\n",
        "\n",
        "        mapping_network.zero_grad()\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "        opt_mapping_network.step()\n",
        "\n",
        "        loop.set_postfix(\n",
        "            gp=gp.item(),\n",
        "            loss_critic=loss_critic.item(),\n",
        "        )\n"
      ],
      "metadata": {
        "id": "eX599MXq65kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = get_loader()\n",
        "\n",
        "gen = Generator(LOG_RESOLUTION, W_DIM).to(DEVICE)\n",
        "critic = Discriminator(LOG_RESOLUTION).to(DEVICE)\n",
        "mapping_network = MappingNetwork(Z_DIM, W_DIM).to(DEVICE)\n",
        "path_length_penalty = PathLengthPenalty(0.99).to(DEVICE)\n",
        "\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
        "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
        "opt_mapping_network = optim.Adam(mapping_network.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
        "\n",
        "gen.train()\n",
        "critic.train()\n",
        "mapping_network.train()"
      ],
      "metadata": {
        "id": "AjlKItBu67iT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54fe3a41-0a7b-4523-d41a-7dc496a57524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MappingNetwork(\n",
              "  (mapping): Sequential(\n",
              "    (0): EqualizedLinear(\n",
              "      (weight): EqualizedWeight()\n",
              "    )\n",
              "    (1): ReLU()\n",
              "    (2): EqualizedLinear(\n",
              "      (weight): EqualizedWeight()\n",
              "    )\n",
              "    (3): ReLU()\n",
              "    (4): EqualizedLinear(\n",
              "      (weight): EqualizedWeight()\n",
              "    )\n",
              "    (5): ReLU()\n",
              "    (6): EqualizedLinear(\n",
              "      (weight): EqualizedWeight()\n",
              "    )\n",
              "    (7): ReLU()\n",
              "    (8): EqualizedLinear(\n",
              "      (weight): EqualizedWeight()\n",
              "    )\n",
              "    (9): ReLU()\n",
              "    (10): EqualizedLinear(\n",
              "      (weight): EqualizedWeight()\n",
              "    )\n",
              "    (11): ReLU()\n",
              "    (12): EqualizedLinear(\n",
              "      (weight): EqualizedWeight()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = get_loader()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_fn(\n",
        "        critic,\n",
        "        gen,\n",
        "        path_length_penalty,\n",
        "        loader,\n",
        "        opt_critic,\n",
        "        opt_gen,\n",
        "        opt_mapping_network,\n",
        "    )\n",
        "    if epoch % 50 == 0:\n",
        "    \tgenerate_examples(gen, epoch)"
      ],
      "metadata": {
        "id": "OCzT8hpk68e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14031e32-1b5f-497f-dfbe-17a0e686292e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/220 [00:00<?, ?it/s]<ipython-input-23-5d0d6474a53b>:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "100%|██████████| 220/220 [04:01<00:00,  1.10s/it, gp=0.182, loss_critic=7.41]\n",
            "100%|██████████| 220/220 [03:59<00:00,  1.09s/it, gp=0.155, loss_critic=-2.8]\n",
            "100%|██████████| 220/220 [03:59<00:00,  1.09s/it, gp=0.0403, loss_critic=23.9]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.14, loss_critic=8.73]\n",
            "100%|██████████| 220/220 [03:59<00:00,  1.09s/it, gp=0.0382, loss_critic=7.35]\n",
            "100%|██████████| 220/220 [03:59<00:00,  1.09s/it, gp=0.00598, loss_critic=11.7]\n",
            "100%|██████████| 220/220 [03:59<00:00,  1.09s/it, gp=0.00448, loss_critic=-7.38]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0803, loss_critic=-2.88]\n",
            "100%|██████████| 220/220 [03:59<00:00,  1.09s/it, gp=0.0116, loss_critic=1.22]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0215, loss_critic=-5.05]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0193, loss_critic=3.68]\n",
            "100%|██████████| 220/220 [04:01<00:00,  1.10s/it, gp=0.0126, loss_critic=2.93]\n",
            "100%|██████████| 220/220 [04:01<00:00,  1.10s/it, gp=0.00622, loss_critic=-3.02]\n",
            "100%|██████████| 220/220 [04:01<00:00,  1.10s/it, gp=0.0548, loss_critic=-7.1]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.10s/it, gp=0.0273, loss_critic=-1.82]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0192, loss_critic=2.38]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0296, loss_critic=-2.26]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0175, loss_critic=1.27]\n",
            "100%|██████████| 220/220 [04:01<00:00,  1.10s/it, gp=0.0587, loss_critic=2.32]\n",
            "100%|██████████| 220/220 [04:01<00:00,  1.10s/it, gp=0.159, loss_critic=4.08]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0695, loss_critic=-0.0404]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0299, loss_critic=-4.87]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0221, loss_critic=-2.95]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.10s/it, gp=0.00488, loss_critic=-1.95]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.10s/it, gp=0.00466, loss_critic=-2.02]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0552, loss_critic=2.65]\n",
            "100%|██████████| 220/220 [04:01<00:00,  1.10s/it, gp=0.0318, loss_critic=-2.75]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0101, loss_critic=-2.55]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.10s/it, gp=0.0393, loss_critic=-0.944]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0185, loss_critic=-4.61]\n",
            "100%|██████████| 220/220 [04:01<00:00,  1.10s/it, gp=0.0147, loss_critic=-3.17]\n",
            "100%|██████████| 220/220 [04:01<00:00,  1.10s/it, gp=0.0288, loss_critic=-3.25]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0178, loss_critic=-4.73]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0315, loss_critic=0.456]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0238, loss_critic=-6.29]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0168, loss_critic=-2.44]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0106, loss_critic=1.06]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0125, loss_critic=-4.9]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.0209, loss_critic=-2.11]\n",
            "100%|██████████| 220/220 [04:00<00:00,  1.09s/it, gp=0.00765, loss_critic=-3.2]\n",
            " 26%|██▋       | 58/220 [01:04<02:51,  1.06s/it, gp=0.0383, loss_critic=-3.57]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bZOm3dCZnmXo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
